# 创建离线同步任务

离线同步适用于批处理场景，支持以下类型的数据同步：

* 单表的全量同步：针对单个表进行完整的数据同步。
* 整库批量表全量同步：对整个数据库中的多个表进行批量的全量数据同步。
* 数据库指定表列表的全量同步：根据指定的表列表，对选定的表进行全量数据同步。

## 创建表到表的离线同步任务

在数据集成的「**数据同步**」页面，用户可以根据业务需求分别设置数据关系、调度配置、过滤规则和运行配置以创建表到表的离线同步任务。参考步骤如下：

1. 切换到「**离线同步**」页面，点击 **新建离线同步** 即可进入创建页面。
2. 输入该离线同步任务的名称。
3. 从“类型”下拉列表中选择离线同步的类型为“表到表”。
4. （可选）输入对该任务的描述。
5. 点击 **下一步** 以进入离线同步任务的配置详情页面。

   <img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/offline-task-detail-tt.png" scope="external" />

6. 设置“数据关系”以定义数据流向。
   
   在「**数据关系**」页面，首先点击 **编辑**，然后分别选择源数据表和目标数据表。完成选择后，点击 **保存** 以确认更改。

7. 添加调度器配置。

   切换到「**调度配置**」页面，点击 **编辑** 并执行如下操作：

   1. 在“调度地址”下拉列表中选择运行该数据流的调度器。

      有关配置调度器的详细信息，请参见 [配置调度系统-添加调度器配置](40.system-configuration.md)。

      <note type="tip">
         <p> 通常情况下，如果没有特殊环境需求，设置一个调度地址即可。</p>   
      </note>
   
   2. 在“工作流名称”下拉框中选择“PyFileBatchWorkflow”。工作流的作用是将作业任务按照指定顺序组织起来。
   3. 在“任务队列”下拉框中选择“PYTHON_FILE_BATCH_QUEUE”。任务队列是轻量级的、动态分配的队列，用于轮询任务。

      <note type="attention">
         <p> 系统初始化时应该已经配置了调度器信息。在当前版本的系统中，调度器的工作流名称和任务队列都是固定配置。在选择调度地址后，系统会自动协助用户完成选择。如果在使用过程中发现缺少这类信息，请联系 OpenPie 技术支持团队获取帮助。</p>   
      </note>

   4. 设置任务执行周期为“立即执行”或者“周期”。
   
       “立即执行”表示只会创建一个任务并执行一次；”周期“表示会按照周期表达式（Cron 表达式）的设置来创建周期任务并定时执行。
       
       对于周期性的任务，可以通过输入 cron 表达式来定义执行的周期。点击“？”可以显示 cron 表达式的格式说明。例如，“*/60 20 1-30 1-12 6”表示“对于 1 月到 12 月，每月的 1 号到 30 号，每个星期六晚上八点，每隔 60 分钟执行一次作业”。

    5. 点击 **保存** 以确认更改。

8. （可选）添加过滤规则。

    切换到「**过滤规则**」页面，首先点击 **编辑**，之后输入 where 条件用于过滤源数据表和目标数据表中的数据。
    
    过滤规则分为如下三类：
    
    * 添加普通条件：需要输入字段名和字段值，并指定逻辑关系。例如“I_quantity > 100”。
    * 添加时间范围条件：需要输入字段名和满足时间范围（年/月）的字段值。例如，字段“I_shipdate”满足时间范围过去“12 月”。
    * 自定义过滤条件：点击 **切换为自定义**，分别输入源表和目标表的自定义 where 表达式（请注意，不要在表达式末尾加分号“;”）。例如，“where update_time > '2024-01-01'”。

    在所有过滤条件添加完成后，点击 **保存** 以确认更改。

9. 修改运行配置。

    “运行配置”展示数据同步任务的系统信息。由于离线同步任务需要做数据校验和文件导出的场景，由此运行配置包括比较算法、差异行数量限制、S3 桶信息、导出文件格式等配置项。

    切换到「**运行配置**」页面，点击 **修改**，并执行如下操作：
    
    1. 在运行配置的编辑页面修改 `s3_config_id` 字段以设置 S3 连接配置信息，之后点击 **完成**。对于其他配置项，可以保持它们的默认设置不变。

       <note type="tip">
          <p>对于批处理类型的导出文件，当前版本默认将其存储路径设置为 S3。用户可以通过数据集成的「总览」页面来添加 S3 配置信息。一旦配置完成，用户就可以在运行配置中通过 s3_config_id 字段选择对应的 S3 配置 ID。如果需要指定除默认配置之外的存储桶，用户还可以通过配置 s3_config_bucket 字段来指定 S3 存储桶，从而覆盖默认的存储桶信息配置。</p>   
       </note>

       有关运行配置项的详细信息，请参见 **离线同步任务运行配置项说明**。

    2. 运行配置修改完成后，点击 **X** 即可返回运行配置页面。
    
    3. 点击 **测试 S3 连接**，如果配置信息无误，系统会显示“连接测试成功”的信息。

10. 在当前离线同步任务的配置详情页面，点击 **运行** 即可开始运行表到表的离线同步任务。

    <img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/terminate-job.png" scope="external" />

如果在点击 **运行** 后就发生配置相关报错，用户可以点击 **返回配置** 并根据报错信息修改相应的配置，之后可以重新运行该任务。在任务运行期间，如果需要强制停止运行该任务，点击 **终止** 即可，相应的调度任务也会被同步删除。

<note type="tip">
   <p> 如果终止了一个正在运行的任务，它可能会在数据导出导入过程中的任意点停止。如果需要在终止后重新启动任务，建议先检查目标表，确认数据是否需要被清空，然后重新创建并启动任务。</p>   
</note>

在表到表的离线同步任务的详情页面，运行结果区域会动态展示离线同步任务在 ExportTable、ImportTable 和 TableDataDiff 各个阶段的执行结果、响应消息、执行时间，以及是否执行成功。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/task-execution-details1.png" scope="external" />

在离线同步任务执行完成后，用户可以点击 **查看调度详情** 来查看执行该任务时详细的调度情况。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/schedule-detail1.png" scope="external" />

对于周期性的表到表的离线同步任务，要查看当前的调度信息，只需切换到「**调度配置**」页面，并点击图标 **>>**，即可显示详细的调度信息。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/current-schedule1.png" scope="external" />

对于周期性的表到表的离线同步任务，如果需要暂停未来的调度周期，只需点击 **暂停调度**，之后可以随时重启暂停的调度。请注意，暂停操作并不会影响当前正在运行的任务。如果决定取消未来的所有调度周期，可以点击 **删除**。这一操作同样不会删除任何正在执行的任务的调度信息。

## 创建库到库的离线同步任务

在数据集成的「**数据同步**」页面，用户可以根据业务需求分别设置数据关系、调度配置、过滤规则和运行配置以创建库到库的离线同步任务。参考步骤如下：

1. 切换到「**离线同步**」页面，点击 **新建离线同步** 即可进入创建页面。
2. 输入该离线同步任务的名称。
3. 从“类型”下拉列表中选择离线同步的类型为“库到库”。
4. （可选）输入对该任务的描述。
5. 点击 **下一步** 以进入离线同步任务的配置详情页面。

   <img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/offline-task-detail-dd.png" scope="external" />

6. 设置“数据关系”以定义数据流向。
   
   在「**数据关系**」页面，点击 **编辑** 并执行如下操作：
   1. 分别选择源数据源和目标数据源。
   2. 选择数据复制方式为“整库复制”或者“多表复制”。
   
       <note type="tip">
            <p>整库复制方式仅适用于源数据库和目标数据库中表名完全相同的情况。如果表名不一致，请选择多表复制方式。</p>   
       </note>

       如果选择“多表复制”，点击 **添加表** 并根据需要选择源表和对应的目标表。
      
   3. 点击 **保存** 以确认更改。

7. 添加调度器配置。

   切换到「**调度配置**」页面，点击 **编辑** 并执行如下操作：

   1. 在“调度地址”下拉列表中选择运行该数据流的调度器。

      有关配置调度器的详细信息，请参见 [配置调度系统-添加调度器配置](40.system-configuration.md)。

      <note type="tip">
         <p> 通常情况下，如果没有特殊环境需求，设置一个调度地址即可。</p>   
      </note>
   
   2. 在“工作流名称”下拉框中选择“PyFileBatchWorkflow”。工作流的作用是将作业任务按照指定顺序组织起来。
   3. 在“任务队列”下拉框中选择“PYTHON_FILE_BATCH_QUEUE”。任务队列是轻量级的、动态分配的队列，用于轮询任务。

      <note type="attention">
         <p> 系统初始化时应该已经配置了调度器信息。在当前版本的系统中，调度器的工作流名称和任务队列都是固定配置。在选择调度地址后，系统会自动协助用户完成选择。如果在使用过程中发现缺少这类信息，请联系 OpenPie 技术支持团队获取帮助。</p>   
      </note>

   4. 设置任务执行周期。库到库的离线同步仅支持“立即执行”，即只会创建一个任务并执行一次。

   5. 点击 **保存** 以确认更改。

8. （可选）添加过滤规则。

    切换到「**过滤规则**」页面，首先点击 **编辑**，之后输入 where 条件以用于过滤源数据表和目标数据表中的数据。
    
    过滤规则分为如下三类：
    
    * 添加普通条件：需要输入字段名和字段值，并指定逻辑关系。例如“I_quantity > 100”。
    * 添加时间范围条件：需要输入字段名和满足时间范围（年/月）的字段值。例如，字段“I_shipdate”满足时间范围过去“12 月”。
    * 自定义过滤条件：点击 **切换为自定义**，分别输入源表和目标表的自定义 where 表达式（请注意，不要在表达式末尾加分号“;”）。例如，“where update_time > '2024-01-01'”。

    在所有过滤条件都添加完成后，点击 **保存** 以确认更改。

9. 修改运行配置。

    “运行配置”展示数据同步任务的系统信息。由于离线同步任务需要做数据校验和文件导出的场景，由此运行配置包括比较算法、差异行数量限制、S3 桶信息、导出文件格式等配置项。

    切换到「**运行配置**」页面，点击 **修改**，并执行如下操作：
    
    1. 在运行配置的编辑页面修改 `s3_config_id` 字段以设置 S3 连接配置信息，之后点击 **完成**。对于其他配置项，可以保持它们的默认设置不变。

       <note type="tip">
          <p>对于批处理类型的导出文件，当前版本默认将其存储路径设置为 S3。用户可以通过数据集成的「总览」页面来添加 S3 配置信息。一旦配置完成，用户就可以在运行配置中通过 s3_config_id 字段选择对应的 S3 配置 ID。如果需要指定除默认配置之外的存储桶，用户还可以通过配置 s3_config_bucket 字段来指定 S3 存储桶，从而覆盖默认的存储桶信息配置。</p>   
       </note>

       有关运行配置项的详细信息，请参见 **离线同步任务运行配置项说明**。

    2. 运行配置修改完成后，点击 **X** 即可返回运行配置页面。
    
    3. 点击 **测试 S3 连接**，如果配置信息无误，系统会显示“连接测试成功”的信息。

10. 在当前离线同步任务的配置详情页面，点击 **运行** 即可开始运行库到库的离线同步任务。

与表到表的离线同步任务不同，库到库的离线同步任务一般会以表为单位拆分成多个子任务，子任务会自动同步到任务列表中并实时显示子任务的执行状态，可能为如下之一：
* 已创建
* 正在运行
* 已完成
* 正在终止
* 正在重启
* 已终止
* 失败
* 数据不一致

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/subtask-list.png" scope="external" />

用户选中库到库的离线同步任务的子任务并单击，即可进入目标子任务的执行详情页面。

对于正在运行中的离线同步子任务，运行结果区域会动态展示离线同步任务在 ExportTable、ImportTable 和 TableDataDiff 各个阶段的执行进度。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/task-in-progress.png" scope="external" />

与表到表的离线同步任务相同，各个子任务的运行结果区域会展示该任务在 ExportTable、ImportTable 和 TableDataDiff 各个阶段的执行结果、响应消息、执行时间，以及是否执行成功。

在离线同步任务执行完成后，用户可以点击 **查看调度详情** 来查看执行该任务时详细的调度情况。

## 离线同步任务运行配置项说明

| 配置项        | 说明                    | 默认值        |
| :----------- | -----------------------| :------------ |
| key_columns   | 表的主键                | 无            |
| extra_columns | 用于比较的非主键列        | 无             |
| algorithm     | 比较算法。取值范围：<ul><li> md5diff：直接计算 MD5 值，且只能确定文件或数据之间是否存在差异。 </li><li>  hashdiff：递归计算 md5 值，并找出具体不同的行。 </li><li>countdiff：通过计算记录数量判断是否有差异。 </li></ul>  | md5diff       |
| limit         | 最多能返回的有差异的行的数量。取值范围 \[1,1000\]。   | 1000  |
| threads       | 执行比较的线程数量    | 1   |
| bisection_threshold| 在 diff 操作期间每个段中的行数阈值。如果段内的行数低于此阈值，则将在内存中直接进行比较。 | 无  |
| bisection_factor | 在每次比较迭代中数据集被划分为校验和片段的数量。该数值应小于 bisection_threshold。| 无  |
| verbose  | 校验时是否输出详细日志    | false   |
| s3_config_id | s3 连接配置信息   | 无   |
| s3_config_bucket | s3 桶的名称。如果为空，则使用默认值。     | 空   |
| mppsql_server_name | mppsql 服务的名称，如果用 Parquet 就是必填项。   | 无   |
| export_batch_rows  | 导出文件被分割成多个文件，每个文件的行数。  | 100000   |
| batch_parallel_size| 批作业并行运行子作业的个数    | 5 |
| batch_file_format  | 批作业的文件格式，支持 csv 和 parquet 两种。   | csv  |
| heartbeat_timeout  | activity 的心跳超时时间。单位：秒。 | 60   |
| export_compress_parallel | 压缩上传任务最大并行数量   | 5   |
| enable_pk_duplicate_detect | 是否开启检测主键重复。开启后，在导入数据前会根据主键进行查重。  | True  |
| source_timezone            | 源库时区。连接源数据库和目标数据库时都使用该时区。   | Asia/Shanghai |
| import_order_by            | 对于大表，将该字段设置为常用于查询过滤条件的字段（例如日期），可以显著提高数据库查询对数据文件的过滤效率，从而提升查询性能。语法和 SQL 的 `ORDER BY` 一致。 | 无  |

## 离线同步任务失败问题排查

在执行同步任务过程中，可能遇到的执行失败的情况，在任务详情页面的运行结果区域，用户可以通过查看任务的状态和执行详情来了解失败的原因。

通常情况下，库到库的离线同步任务很少需要再次执行，除非失败原因是由于资源、环境等不可控因素引起的，在这种情况下，任务可以被重新执行。

如果任务执行失败是由于系统稳定性、数据库状态等服务质量问题引起的，用户可以在任务详情页面的运行结果区域，找到当前执行的任务记录以及执行失败的具体步骤，并点击该步骤下方的 **重试** 来重新执行该步骤。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/task-retry1.png" scope="external" />

如果任务失败是由于配置问题引起的，那么需要重新修改任务配置。用户可以点击 **返回配置** 并根据报错信息修改相应的配置，之后可以重新运行该任务。

在执行同步任务时，通常不会出现数据不一致的情况。目前，数据不一致的最主要原因是重复导入数据，而次要原因可能是数据导出、导入或数据校验流程中出现了问题。如果任务状态显示“数据不一致”，建议首先清理目标数据源的数据，然后确认环境配置等的正确性，最后重新执行任务。如果执行上述操作后数据仍然不一致，请联系 OpenPie 技术支持团队。

此外，用户可以通过审计日志来了解任务的方法调用情况。在 PieDataCS 云原生平台的数仓操作界面，点击菜单栏「**审计日志**」即可进入该功能页面。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/task-log.png" scope="external" />

## 清理源数据

对于离线同步任务，用户可以根据设定的周期和 where 条件来实现定时定量的数据迁移。数据迁移完成后，如果用户希望清理源数据库的表中的特定数据，可以通过执行清理源数据的操作来实现。

在数据集成的「**数据同步**」页面，用户点击 **数据清理** 即可进入清理数据的功能页面。该页面以列表形式展示了当前所有的离线同步任务。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/data-clean-page1.png" scope="external" />

在「**清理数据**」页下的离线任务列表中的目标任务的“源数据状态”栏下，用户可以点击 **清理源数据**，并基于 where 条件来按需清理目标源数据，如下图所示。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/clean-source-data-where.png" scope="external" />

对于数据量较大的场景，为了保证源数据库的稳定性，可以实现源数据的分批多轮自动清理。默认情况下，每轮删除的数据量为 10000 条，但用户也可以输入自定义的整数值来设定每轮删除的数据量。

如果未设置 where 条件，源数据清理操作将会清空整张表的所有数据。

源数据清理完成后，系统会显示“已清理”。如果用户执行了清理数据源的操作，可以点击 **查看清理历史** 来查看历史清理信息，其中包括调度器的 IP 地址、执行的 SQL 语句、当前用户名称、清理时间以及本次清理所删除的记录条数等详细信息。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/clean-history1.png" scope="external" />

## 清理导出文件

对于离线同步任务，数据迁移完成后或在迁移过程中，如果通过数据校验失败等方式发现了导出数据的问题，用户可以清理这些有问题的导出文件。

在数据集成的「**数据同步**」页面，用户点击 **数据清理** 即可进入清理数据的功能页面。该页面以列表形式展示了当前所有的离线同步任务。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/data-clean-page1.png" scope="external" />

在「**清理数据**」页下的离线任务列表中的目标任务的“导出文件状态”栏下，用户可以点击 **清理**，确认后即可清理目标导出文件。

<img src="https://pdb-doc.oss-cn-beijing.aliyuncs.com/dataflow/v2/clean-export-file1.png" scope="external" />

文件清理成功后，系统会显示“已删除”。如果导出文件状态栏显示为“不存在”，则表明该任务没有生成导出文件。
